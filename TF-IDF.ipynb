{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP2tFQWlB3Enf5Zy++YU1t4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LA5QNJta50zK","collapsed":true},"outputs":[],"source":["# --- ステップ1: Python 3.10の準備 ---\n","!sudo apt-get update -y\n","!sudo apt-get install -y python3.10 python3.10-distutils\n","!curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10\n","print(\"--- Python 3.10の準備完了 ---\")\n","\n","# --- ステップ2: MeCab本体と関連ライブラリのインストール ---\n","!sudo apt-get install -y mecab libmecab-dev file\n","!python3.10 -m pip install mecab-python3\n","print(\"--- MeCab本体のインストール完了 ---\")\n","\n","# --- ステップ3: mecab-ipadic-neologd のインストール ---\n","# GitHubからソースコードをクローン（ダウンロード）\n","!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n","# ダウンロードしたディレクトリに移動して、インストールスクリプトを実行\n","!echo \"yes\" | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a\n","\n","print(\"\\n--- mecab-ipadic-neologdのインストールが完了しました ---\")"]},{"cell_type":"code","source":["#ここは自分が使うものだけインストールすればいい\n","!python3.10 -m pip install google-cloud-translate pandas openpyxl wikiextractor\n","print(\"\\n--- 追加ライブラリのインストールが完了しました ---\")"],"metadata":{"id":"F9gmyR7i59kh","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 日本語版Wikipediaの最新ダンプをダウンロード\n","!wget https://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz2"],"metadata":{"id":"0KhAPric6CgH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --json オプションを追加して、出力がJSON形式になるように修正。JSON形式にしてるけど自分が使うツールに合わせて変更してもいいかも\n","!python3.10 -m wikiextractor.WikiExtractor jawiki-latest-pages-articles.xml.bz2 -o extracted_wiki -b 10M --processes 8 --json"],"metadata":{"id":"z8tZkZcr6Exy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#もし自分が何かしらのデータ（文章とか、単語とかを集めたエクセルファイルで計算する場合）持ってるならそのデータファイルを読み込めるセル。データファイル読み込ませたかったら今S2_data.xlsxになってるところ自分のエクセルファイルに置き換えればいい（Google認証のところは全部削除）。\n","from google.colab import files\n","import os\n","import re\n","import json\n","from collections import Counter\n","\n","\n","print(\"Excelファイル (S2_data.xlsx) と Google Cloud認証キー (JSONファイル) をアップロードしてください。\")\n","uploaded = files.upload()\n","\n","# アップロードされたファイル名を取得\n","excel_file_name = \"S2_data.xlsx\"\n","\n","#以下無視\n","gcp_key_file_name = \"\"\n","for fn in uploaded.keys():\n","  if fn.endswith('.json'):\n","    gcp_key_file_name = fn\n","    print(f\"認証キー '{fn}' を認識しました。\")\n","\n","if not gcp_key_file_name:\n","    print(\"エラー: Google Cloudの認証キー(JSONファイル)が見つかりません。\")\n","\n","# 環境変数を設定してPythonから認証キーを使えるようにする\n","os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = gcp_key_file_name"],"metadata":{"id":"zcU1lFXn6Hyj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#単語辞書作成。作成された辞書json形式でGoogle Driveのマイドライブってとこに格納されるようにしてる。\n","%%writefile create_freq_dict.py\n","\n","import os\n","import re\n","import json\n","from collections import Counter\n","import MeCab\n","\n","\n","print(\"Wikipediaの全テキストから単語頻度辞書を作成します...\")\n","\n","# NEologd辞書と、mecabrc設定ファイルのパスを明示的に指定します。\n","NEOLOGD_DIC_PATH = \"/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n","MECABRC_PATH = \"/etc/mecabrc\"\n","\n","try:\n","    # 辞書パスと設定ファイルパスの両方を指定\n","    tagger_options = f\"-d {NEOLOGD_DIC_PATH} -r {MECABRC_PATH}\"\n","    wakati_tagger = MeCab.Tagger(f\"-Owakati {tagger_options}\")\n","    wakati_tagger.parse('') # 初期化チェック\n","    print(\"MeCab Tagger (NEologd) の初期化に成功しました。\")\n","except Exception as e:\n","    print(f\"MeCab Tagger (NEologd) の初期化に失敗しました。エラー: {e}\")\n","    # フォールバックもrcfileを指定\n","    try:\n","        print(\"フォールバックとしてデフォルト辞書での初期化を試みます...\")\n","        wakati_tagger = MeCab.Tagger(f\"-Owakati -r {MECABRC_PATH}\")\n","        wakati_tagger.parse('')\n","        print(\"デフォルト辞書での初期化に成功しました。\")\n","    except Exception as e2:\n","        print(f\"デフォルト辞書での初期化にも失敗しました。エラー: {e2}\")\n","        raise RuntimeError(\"MeCabを正常に初期化できませんでした。\")\n","\n","# --- 以降の処理 ---\n","word_counter = Counter()\n","total_word_count = 0\n","input_dir = 'extracted_wiki'\n","\n","file_paths = [os.path.join(root, file) for root, _, files in os.walk(input_dir) for file in files if file.startswith('wiki_')]\n","file_count = len(file_paths)\n","\n","for i, filepath in enumerate(file_paths):\n","    print(f\"  - 処理中: {filepath} ({i+1}/{file_count})\")\n","    with open(filepath, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            try:\n","                article = json.loads(line)\n","                text = article.get('text', '')\n","                text = re.sub(r'<.*?>', '', text)\n","                text = re.sub(r'\\[\\[.*?\\|(.*?)\\]\\]', r'\\1', text)\n","                text = re.sub(r'\\[\\[(.*?)\\]\\]', r'\\1', text)\n","                tokens = wakati_tagger.parse(text).strip().split()\n","                word_counter.update(tokens)\n","                total_word_count += len(tokens)\n","            except json.JSONDecodeError:\n","                continue\n","\n","print(\"\\n単語頻度辞書の作成が完了しました。\")\n","print(f\"ユニーク単語数: {len(word_counter)}\")\n","print(f\"総単語数: {total_word_count}\")\n","\n","# 結果をGoogleドライブに保存\n","output_freq_file = '/content/drive/MyDrive/wiki_freq_neologd.json'\n","with open(output_freq_file, 'w', encoding='utf-8') as f:\n","    json.dump({\n","        'freq_map': dict(word_counter),\n","        'total_words': total_word_count\n","    }, f, ensure_ascii=False, indent=2)\n","\n","print(f\"\\n頻度データをあなたのGoogleドライブ ('{output_freq_file}') に保存しました。\")\n","print(\"これ以降は、「ノートブック②：メイン分析用」に切り替えてください。\")"],"metadata":{"id":"gtuUURBO6Kxu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#前のセル実行\n","!python3.10 create_freq_dict.py"],"metadata":{"id":"rEs64dP09JvC","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ここで自分が用意したデータファイルの単語とTF-IDFの計算してる。英訳機能とかスコアのパーセンテージ化機能（Softmax）とか含まれてるからそれらに関するところは全部削除で大丈夫（無視って書いてるとこ削除、他もいらんとこあるかも。削除したらAIにエラー起きんように全体修正してもらえばいい）。品詞の設定も自分が用いる品詞に適宜変更して。\n","%%writefile create_json.py\n","import pandas as pd\n","import MeCab\n","from collections import Counter, defaultdict\n","import json\n","import os\n","import math\n","import time\n","from google.cloud import translate_v2 as translate\n","import html # ★★★ HTMLデコードライブラリをインポート ★★★\n","\n","# --- 設定項目 ---\n","# 1. MeCabの辞書パス\n","MECAB_DIC_PATH = \"/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n","\n","# 2. ジャンルIDマップ。無視\n","GENRE_ID_MAP = {\n","    \"文芸・評論\": \"bungei\", \"ノンフィクション\": \"nonfiction\", \"ビジネス・経済\": \"business\",\n","    \"歴史・地理\": \"rekishi\", \"政治・社会\": \"politics\", \"芸能・エンターテインメント\": \"entertainment\",\n","    \"アート・建築・デザイン\": \"art\", \"人文・思想・宗教\": \"humanities\", \"暮らし・健康・料理\": \"life\",\n","    \"サイエンス・テクノロジー\": \"science\", \"趣味・実用\": \"hobby\", \"教育・自己啓発\": \"education\",\n","    \"スポーツ・アウトドア\": \"sports\", \"事典・年鑑・本・ことば\": \"dictionary\", \"音楽\": \"music\",\n","    \"旅行・紀行\": \"ryokou\", \"絵本・児童書\": \"kids\", \"コミックス\": \"comics\"\n","}\n","\n","# 3. Google CloudプロジェクトID。無視\n","GCP_PROJECT_ID = \"my-translation-app-463407\"  # ご自身のIDに設定してください\n","\n","# 4. 文の翻訳を有効にするか。無視\n","TRANSLATE_SENTENCES = True\n","\n","# 5. 読み込む頻度データファイル。さっき作ったファイル。同じディレクトリに移動させて。それか/content/drive/MyDrive/wiki_freq_neologd.jsonに置き換える。\n","WIKI_FREQ_FILE = \"wiki_freq_neologd.json\"\n","\n","# 6. ファイル設定\n","EXCEL_FILE = \"S2_data.xlsx\"\n","SHEET_NAMES = list(GENRE_ID_MAP.keys())\n","OUTPUT_JSON_FILE = \"analysis_data.json\"\n","ROW_START, ROW_END = 1, 11\n","\n","# 7. Softmax温度設定。無視\n","SOFTMAX_TEMPERATURE = 15.0\n","\n","# --- クライアントとTaggerの初期化 ---\n","try:\n","    translate_client = translate.Client()\n","except Exception as e:\n","    print(f\"Google翻訳クライアントの初期化に失敗。認証情報を確認してください。エラー: {e}\")\n","    exit()\n","\n","try:\n","    tagger_options = f'-d \"{MECAB_DIC_PATH}\"'\n","    tagger = MeCab.Tagger(tagger_options)\n","    wakati_tagger = MeCab.Tagger(f\"-Owakati {tagger_options}\")\n","    tagger.parse('')\n","    print(\"MeCab Tagger (NEologd) の初期化に成功しました。\")\n","except RuntimeError as e:\n","    print(f\"MeCabの初期化に失敗しました。: {e}\")\n","    print(f\"MECAB_DIC_PATH '{MECAB_DIC_PATH}' が正しいか確認してください。\")\n","    exit()\n","\n","# --- 関数定義 ---\n","def load_freq_data(filepath):\n","    print(f\"ステップ1/5: 外部単語頻度データ '{filepath}' を読み込み中...\")\n","    if not os.path.exists(filepath):\n","        print(f\"  - エラー: ファイル '{filepath}' が見つかりません。\")\n","        exit()\n","    with open(filepath, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","    if 'freq_map' not in data or 'total_words' not in data:\n","        print(f\"  - エラー: '{filepath}' は 'freq_map' と 'total_words' のキーを持つ必要があります。\")\n","        exit()\n","    print(f\"  - 完了。{len(data['freq_map'])}語のデータを読み込みました。\")\n","    return data['freq_map'], data['total_words']\n","\n","#無視\n","def translate_texts(texts, target_language=\"en\"):\n","    \"\"\"テキストのリストを翻訳し、HTMLエンティティをデコードする\"\"\"\n","    if not texts: return {}\n","    unique_texts = list(set(text for text in texts if text and isinstance(text, str)))\n","    if not unique_texts: return {text: \"\" for text in texts}\n","\n","    BATCH_SIZE, all_translations = 128, {}\n","    print(f\"  - 合計 {len(unique_texts)}件のユニークなテキストを翻訳します...\")\n","    for i in range(0, len(unique_texts), BATCH_SIZE):\n","        chunk = unique_texts[i:i + BATCH_SIZE]\n","        print(f\"    - 翻訳中... (チャンク {i//BATCH_SIZE + 1} / {math.ceil(len(unique_texts)/BATCH_SIZE)})\")\n","        try:\n","            results = translate_client.translate(chunk, target_language=target_language)\n","            # ★★★ HTMLエンティティをデコードする処理を追加 ★★★\n","            for item in results:\n","                decoded_text = html.unescape(item['translatedText'])\n","                all_translations[item['input']] = decoded_text\n","        except Exception as e:\n","            print(f\"      - 翻訳API呼び出し中にエラー: {e}\")\n","            for text in chunk: all_translations[text] = \"\"\n","        time.sleep(0.1)\n","    return all_translations\n","\n","#無視\n","def apply_softmax(word_objects, temperature=1.0):\n","    if not word_objects: return []\n","    if temperature <= 0: temperature = 1.0\n","    scores = [item['score'] / temperature for item in word_objects]\n","    max_score = max(scores)\n","    exps = [math.exp(s - max_score) for s in scores]\n","    sum_exps = sum(exps)\n","    if sum_exps == 0: return word_objects\n","    for i, item in enumerate(word_objects):\n","        item['score'] = exps[i] / sum_exps\n","    return word_objects\n","\n","def get_tokens_from_text(text):\n","    if not isinstance(text, str): text = str(text)\n","    return wakati_tagger.parse(text).strip().split()\n","\n","def analyze_pos_and_score(text, freq_map, total_corpus_words):\n","    if not isinstance(text, str): text = str(text)\n","\n","    tokens_for_tf = get_tokens_from_text(text)\n","    tf = Counter(tokens_for_tf)\n","    node = tagger.parseToNode(text)\n","    results = defaultdict(list)\n","\n","    max_idf = math.log((total_corpus_words / 1) + 1)\n","\n","    while node:\n","        surface = node.surface\n","        features = node.feature.split(',')\n","        if not surface or len(features) < 2:\n","            node = node.next\n","            continue\n","\n","        freq_in_doc = tf.get(surface, 0)\n","        if freq_in_doc == 0:\n","            node = node.next\n","            continue\n","\n","        freq_in_corpus = freq_map.get(surface)\n","        idf = max_idf if freq_in_corpus is None else math.log((total_corpus_words / (freq_in_corpus + 1)) + 1)\n","\n","        score = (freq_in_doc * idf) + math.log(freq_in_doc + 1)\n","        word_obj = {\"word\": surface, \"score\": score}\n","\n","        pos, sub_pos1 = features[0], features[1]\n","\n","        if pos == \"名詞\":\n","            if sub_pos1 in [\"代名詞\", \"非自立\", \"数\", \"接尾\", \"副詞可能\", \"形容動詞語幹\"]:\n","                results[\"nouns_functional\"].append(word_obj)\n","            else:\n","                results[\"nouns_other\"].append(word_obj)\n","        elif pos == \"動詞\":\n","            results[\"verbs\"].append(word_obj)\n","        elif pos == \"形容詞\":\n","            results[\"adjectives\"].append(word_obj)\n","        elif pos == \"形容動詞\":\n","            results[\"adjectival_verbs\"].append(word_obj)\n","\n","        node = node.next\n","    return results\n","\n","#無視\n","def extract_important_words(analysis_results_raw):\n","    important = defaultdict(list)\n","    for pos_key, words in analysis_results_raw.items():\n","        word_counts = Counter(item['word'] for item in words)\n","        important_word_names = {word for word, count in word_counts.items() if count >= 2}\n","\n","        important_words = [item for item in words if item['word'] in important_word_names]\n","        important[pos_key] = important_words\n","    return important\n","\n","#無視\n","def extract_important_sentences_by_score(text, word_score_map):\n","    if not isinstance(text, str): return \"説明文がテキスト形式ではありません\"\n","\n","    sentences = [s.strip() for s in text.split(\"。\") if s.strip()]\n","    if not sentences: return \"該当する文がありません\"\n","\n","    scored_sentences = []\n","    for sentence in sentences:\n","        tokens = get_tokens_from_text(sentence)\n","        sentence_score = sum(word_score_map.get(token, 0) for token in tokens)\n","        scored_sentences.append((sentence, sentence_score))\n","\n","    scored_sentences = [s for s in scored_sentences if s[1] > 0]\n","    if not scored_sentences: return \"重要度の高い単語を含む文がありません\"\n","\n","    scored_sentences.sort(key=lambda x: x[1], reverse=True)\n","    return scored_sentences[0][0] + \"。\"\n","\n","# --- メイン処理 ---\n","def main():\n","    freq_map, total_words = load_freq_data(WIKI_FREQ_FILE)\n","\n","    print(\"ステップ2/5: 全てのドキュメントをExcelから読み込み中...\")\n","    all_docs_raw = []\n","    for sheet_name in SHEET_NAMES:\n","        try:\n","            df = pd.read_excel(EXCEL_FILE, sheet_name=sheet_name, usecols=[1, 3], header=None, engine=\"openpyxl\")\n","            for i in range(ROW_START, ROW_END):\n","                if i < len(df):\n","                    all_docs_raw.append({\n","                        \"sheet_name\": sheet_name,\n","                        \"title\": str(df.iloc[i, 0]),\n","                        \"text\": str(df.iloc[i, 1])\n","                    })\n","        except Exception as e:\n","            print(f\"  - シート '{sheet_name}' の読み込みエラー: {e}\")\n","\n","    print(\"ステップ3/5: 各ドキュメントを解析し、スコアを計算中...\")\n","    all_docs_processed = []\n","    for i, doc in enumerate(all_docs_raw):\n","        analysis_results_raw = analyze_pos_and_score(doc['text'], freq_map, total_words)\n","\n","        aggregated_analysis = defaultdict(lambda: defaultdict(float))\n","        word_score_map = {}\n","        for category, word_list in analysis_results_raw.items():\n","            for item in word_list:\n","                aggregated_analysis[category][item['word']] += item['score']\n","        for category_scores in aggregated_analysis.values():\n","            for word, score in category_scores.items():\n","                word_score_map[word] = score\n","\n","        final_analysis_results = {\n","            cat: apply_softmax([{\"word\": w, \"score\": s} for w, s in scores.items()], temperature=SOFTMAX_TEMPERATURE)\n","            for cat, scores in aggregated_analysis.items()\n","        }\n","\n","        important_words_raw = extract_important_words(analysis_results_raw)\n","        aggregated_important = defaultdict(lambda: defaultdict(float))\n","        for category, word_list in important_words_raw.items():\n","            for item in word_list:\n","                aggregated_important[category][item['word']] += item['score']\n","\n","        final_important_words = {\n","            cat: apply_softmax([{\"word\": w, \"score\": s} for w, s in scores.items()], temperature=SOFTMAX_TEMPERATURE)\n","            for cat, scores in aggregated_important.items()\n","        }\n","\n","        doc.update({\n","            \"genre_id\": GENRE_ID_MAP.get(doc['sheet_name'], doc['sheet_name'].lower()),\n","            \"title_id\": f\"title{(i % (ROW_END - ROW_START)) + 1:02d}\",\n","            \"important_sentence\": extract_important_sentences_by_score(doc['text'], word_score_map),\n","            \"analysis\": final_analysis_results,\n","            \"important_words\": final_important_words\n","        })\n","        all_docs_processed.append(doc)\n","\n","#無視\n","    print(\"ステップ4/5: テキストを翻訳中...\")\n","    words_to_translate, sentences_to_translate = set(), []\n","    for doc in all_docs_processed:\n","        if TRANSLATE_SENTENCES:\n","            sentences_to_translate.append(doc['text'])\n","            sentences_to_translate.append(doc['important_sentence'])\n","        for category in doc['analysis']:\n","            for item in doc['analysis'][category]:\n","                words_to_translate.add(item['word'])\n","        for category in doc['important_words']:\n","            for item in doc['important_words'][category]:\n","                words_to_translate.add(item['word'])\n","\n","    word_translation_map = translate_texts(list(words_to_translate))\n","    sentence_translation_map = translate_texts(sentences_to_translate) if TRANSLATE_SENTENCES else {}\n","\n","    print(\"ステップ5/5: 最終的なJSONデータを構築中...\")\n","    final_data = defaultdict(list)\n","    for doc in all_docs_processed:\n","        for category in doc['analysis']:\n","            for item in doc['analysis'][category]:\n","                item['translation'] = word_translation_map.get(item['word'], \"\")\n","        for category in doc['important_words']:\n","            for item in doc['important_words'][category]:\n","                item['translation'] = word_translation_map.get(item['word'], \"\")\n","\n","        doc['text_en'] = sentence_translation_map.get(doc['text'], \"\")\n","        doc['important_sentence_en'] = sentence_translation_map.get(doc['important_sentence'], \"\")\n","\n","        doc.pop('sheet_name', None)\n","        final_data[doc['genre_id']].append(doc)\n","\n","    id_to_genre_map = {v: k for k, v in GENRE_ID_MAP.items()}\n","    final_json_data = {id_to_genre_map.get(gid, gid): docs for gid, docs in final_data.items()}\n","\n","    with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f:\n","        json.dump(final_json_data, f, ensure_ascii=False, indent=2)\n","\n","    print(f\"\\n処理完了。'{OUTPUT_JSON_FILE}' を保存しました。\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"muGCi25hXGBC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#前のセル実行\n","!python3.10 create_json.py"],"metadata":{"id":"whzqZUp8XTa_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#これは無視\n","!cp -r extracted_wiki /content/drive/MyDrive/\n","!cp -r jawiki-latest-pages-articles.xml.bz2 /content/drive/MyDrive/"],"metadata":{"id":"BqgPH0alHFoo"},"execution_count":null,"outputs":[]}]}