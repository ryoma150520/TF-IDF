{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1-WHMDrHwmak9fwGe9hLPNosY6C0JLMIN",
      "authorship_tag": "ABX9TyMoJc+ffo8BAvxoGbW2Oc9i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryoma150520/TF-IDF/blob/main/TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA5QNJta50zK",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#セル1\n",
        "# --- ステップ1: Python 3.10の準備 ---\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y python3.10 python3.10-distutils\n",
        "!curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10\n",
        "print(\"--- Python 3.10の準備完了 ---\")\n",
        "\n",
        "# --- ステップ2: MeCab本体と関連ライブラリのインストール ---\n",
        "!sudo apt-get install -y mecab libmecab-dev file\n",
        "!python3.10 -m pip install mecab-python3\n",
        "print(\"--- MeCab本体のインストール完了 ---\")\n",
        "\n",
        "# --- ステップ3: mecab-ipadic-neologd のインストール ---\n",
        "# GitHubからソースコードをクローン（ダウンロード）\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "# ダウンロードしたディレクトリに移動して、インストールスクリプトを実行\n",
        "!echo \"yes\" | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a\n",
        "\n",
        "print(\"\\n--- mecab-ipadic-neologdのインストールが完了しました ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#セル2\n",
        "!python3.10 -m pip install google-cloud-translate pandas openpyxl wikiextractor\n",
        "print(\"\\n--- 追加ライブラリのインストールが完了しました ---\")"
      ],
      "metadata": {
        "id": "F9gmyR7i59kh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#セル3\n",
        "# 日本語版Wikipediaの最新ダンプをダウンロード\n",
        "!wget https://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz2"
      ],
      "metadata": {
        "id": "0KhAPric6CgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#セル4\n",
        "# --json オプションを追加して、出力がJSON形式になるように修正。JSON形式にしてるけど自分が使うツールに合わせて変更してもいいかも\n",
        "!python3.10 -m wikiextractor.WikiExtractor jawiki-latest-pages-articles.xml.bz2 -o extracted_wiki -b 10M --processes 8 --json"
      ],
      "metadata": {
        "id": "z8tZkZcr6Exy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#セル5.1\n",
        "#文書頻度\n",
        "%%writefile create_freq_dict.py\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "import MeCab\n",
        "\n",
        "print(\"Wikipediaの全テキストから【文書頻度(Document Frequency)】辞書を作成します...\")\n",
        "\n",
        "# NEologd辞書と、mecabrc設定ファイルのパスを明示的に指定します。\n",
        "NEOLOGD_DIC_PATH = \"/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
        "MECABRC_PATH = \"/etc/mecabrc\"\n",
        "\n",
        "try:\n",
        "    # 辞書パスと設定ファイルパスの両方を指定\n",
        "    tagger_options = f\"-d {NEOLOGD_DIC_PATH} -r {MECABRC_PATH}\"\n",
        "    wakati_tagger = MeCab.Tagger(f\"-Owakati {tagger_options}\")\n",
        "    wakati_tagger.parse('') # 初期化チェック\n",
        "    print(\"MeCab Tagger (NEologd) の初期化に成功しました。\")\n",
        "except Exception as e:\n",
        "    print(f\"MeCab Tagger (NEologd) の初期化に失敗しました。エラー: {e}\")\n",
        "    # フォールバックもrcfileを指定\n",
        "    try:\n",
        "        print(\"フォールバックとしてデフォルト辞書での初期化を試みます...\")\n",
        "        wakati_tagger = MeCab.Tagger(f\"-Owakati -r {MECABRC_PATH}\")\n",
        "        wakati_tagger.parse('')\n",
        "        print(\"デフォルト辞書での初期化に成功しました。\")\n",
        "    except Exception as e2:\n",
        "        print(f\"デフォルト辞書での初期化にも失敗しました。エラー: {e2}\")\n",
        "        raise RuntimeError(\"MeCabを正常に初期化できませんでした。\")\n",
        "\n",
        "# --- ★★★ 修正点1: 変数名を役割に合わせて変更 ★★★ ---\n",
        "doc_freq_counter = Counter() # 単語が出現した「文書数」をカウントする\n",
        "total_document_count = 0     # 処理した「文書数」をカウントする\n",
        "input_dir = 'extracted_wiki'\n",
        "\n",
        "file_paths = [os.path.join(root, file) for root, _, files in os.walk(input_dir) for file in files if file.startswith('wiki_')]\n",
        "file_count = len(file_paths)\n",
        "\n",
        "for i, filepath in enumerate(file_paths):\n",
        "    print(f\"  - 処理中: {filepath} ({i+1}/{file_count})\")\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            # 各行が1つの文書（記事）に対応する\n",
        "            try:\n",
        "                article = json.loads(line)\n",
        "                text = article.get('text', '')\n",
        "\n",
        "                # 簡単なテキストクリーニング\n",
        "                text = re.sub(r'<.*?>', '', text)\n",
        "                text = re.sub(r'\\[\\[.*?\\|(.*?)\\]\\]', r'\\1', text)\n",
        "                text = re.sub(r'\\[\\[(.*?)\\]\\]', r'\\1', text)\n",
        "\n",
        "                tokens = wakati_tagger.parse(text).strip().split()\n",
        "\n",
        "                # --- ★★★ 修正点2: この文書のユニークな単語のみをカウント ★★★ ---\n",
        "                unique_tokens = set(tokens)\n",
        "                doc_freq_counter.update(unique_tokens)\n",
        "\n",
        "                # --- ★★★ 修正点3: 文書数をカウントアップ ★★★ ---\n",
        "                total_document_count += 1\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "\n",
        "print(\"\\n文書頻度辞書の作成が完了しました。\")\n",
        "print(f\"ユニーク単語数: {len(doc_freq_counter)}\")\n",
        "print(f\"総文書数（記事数）: {total_document_count}\")\n",
        "\n",
        "# --- ★★★ 修正点4: 新しいファイル名とJSON構造で結果を保存 ★★★ ---\n",
        "output_idf_file = '/content/drive/MyDrive/wiki_tfidf_data.json'\n",
        "with open(output_idf_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump({\n",
        "        'total_documents': total_document_count,\n",
        "        'doc_freq_map': dict(doc_freq_counter)\n",
        "    }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\n文書頻度データをあなたのGoogleドライブ ('{output_idf_file}') に保存しました。\")\n",
        "print(\"これ以降は、メインの分析スクリプト(create_json.py)もこの新しいJSONファイルを読むように修正する必要があります。\")"
      ],
      "metadata": {
        "id": "TrIH4skvlUpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#セル5.2\n",
        "#単語頻度\n",
        "%%writefile create_freq_dict.py\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "import MeCab\n",
        "\n",
        "\n",
        "print(\"Wikipediaの全テキストから単語頻度辞書を作成します...\")\n",
        "\n",
        "# NEologd辞書と、mecabrc設定ファイルのパスを明示的に指定します。\n",
        "NEOLOGD_DIC_PATH = \"/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
        "MECABRC_PATH = \"/etc/mecabrc\"\n",
        "\n",
        "try:\n",
        "    # 辞書パスと設定ファイルパスの両方を指定\n",
        "    tagger_options = f\"-d {NEOLOGD_DIC_PATH} -r {MECABRC_PATH}\"\n",
        "    wakati_tagger = MeCab.Tagger(f\"-Owakati {tagger_options}\")\n",
        "    wakati_tagger.parse('') # 初期化チェック\n",
        "    print(\"MeCab Tagger (NEologd) の初期化に成功しました。\")\n",
        "except Exception as e:\n",
        "    print(f\"MeCab Tagger (NEologd) の初期化に失敗しました。エラー: {e}\")\n",
        "    # フォールバックもrcfileを指定\n",
        "    try:\n",
        "        print(\"フォールバックとしてデフォルト辞書での初期化を試みます...\")\n",
        "        wakati_tagger = MeCab.Tagger(f\"-Owakati -r {MECABRC_PATH}\")\n",
        "        wakati_tagger.parse('')\n",
        "        print(\"デフォルト辞書での初期化に成功しました。\")\n",
        "    except Exception as e2:\n",
        "        print(f\"デフォルト辞書での初期化にも失敗しました。エラー: {e2}\")\n",
        "        raise RuntimeError(\"MeCabを正常に初期化できませんでした。\")\n",
        "\n",
        "# --- 以降の処理 ---\n",
        "word_counter = Counter()\n",
        "total_word_count = 0\n",
        "input_dir = 'extracted_wiki'\n",
        "\n",
        "file_paths = [os.path.join(root, file) for root, _, files in os.walk(input_dir) for file in files if file.startswith('wiki_')]\n",
        "file_count = len(file_paths)\n",
        "\n",
        "for i, filepath in enumerate(file_paths):\n",
        "    print(f\"  - 処理中: {filepath} ({i+1}/{file_count})\")\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                article = json.loads(line)\n",
        "                text = article.get('text', '')\n",
        "                text = re.sub(r'<.*?>', '', text)\n",
        "                text = re.sub(r'\\[\\[.*?\\|(.*?)\\]\\]', r'\\1', text)\n",
        "                text = re.sub(r'\\[\\[(.*?)\\]\\]', r'\\1', text)\n",
        "                tokens = wakati_tagger.parse(text).strip().split()\n",
        "                word_counter.update(tokens)\n",
        "                total_word_count += len(tokens)\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "\n",
        "print(\"\\n単語頻度辞書の作成が完了しました。\")\n",
        "print(f\"ユニーク単語数: {len(word_counter)}\")\n",
        "print(f\"総単語数: {total_word_count}\")\n",
        "\n",
        "# 結果をGoogleドライブに保存\n",
        "output_freq_file = '/content/drive/MyDrive/wiki_freq_neologd.json'\n",
        "with open(output_freq_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump({\n",
        "        'freq_map': dict(word_counter),\n",
        "        'total_words': total_word_count\n",
        "    }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\n頻度データをあなたのGoogleドライブ ('{output_freq_file}') に保存しました。\")\n",
        "print(\"これ以降は、「ノートブック②：メイン分析用」に切り替えてください。\")"
      ],
      "metadata": {
        "id": "gtuUURBO6Kxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#セル6\n",
        "#前のセル実行\n",
        "!python3.10 create_freq_dict.py"
      ],
      "metadata": {
        "id": "rEs64dP09JvC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF計算\n",
        "%%writefile tfidf.py\n",
        "import json\n",
        "import MeCab\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# --- ステップ1: 設定（ここを編集してください） ---\n",
        "\n",
        "# 解析したいテキストを入力\n",
        "INPUT_TEXT = \"今日は良い天気ですね。\"\n",
        "\n",
        "\n",
        "# 完全なIDF計算に使用するデータファイル。ファイルのところでアップロードするか/content/drive/MyDrive/wiki_tfidf_data.jsonに置き換える。\n",
        "IDF_DATA_FILE = \"wiki_tfidf_data.json\"\n",
        "\n",
        "\n",
        "# --- ステップ2: MeCabとIDFデータの準備 ---\n",
        "\n",
        "# MeCab Taggerの初期化\n",
        "# NEologd辞書と、mecabrc設定ファイルのパスを明示的に指定します。\n",
        "NEOLOGD_DIC_PATH = \"/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
        "MECABRC_PATH = \"/etc/mecabrc\"\n",
        "\n",
        "try:\n",
        "    # 辞書パスと設定ファイルパスの両方を指定\n",
        "    tagger_options = f\"-d {NEOLOGD_DIC_PATH} -r {MECABRC_PATH}\"\n",
        "    wakati_tagger = MeCab.Tagger(f\"-Owakati {tagger_options}\")\n",
        "    wakati_tagger.parse('') # 初期化チェック\n",
        "    print(\"MeCab Tagger (NEologd) の初期化に成功しました。\")\n",
        "except Exception as e:\n",
        "    print(f\"MeCab Tagger (NEologd) の初期化に失敗しました。エラー: {e}\")\n",
        "    # フォールバックもrcfileを指定\n",
        "    try:\n",
        "        print(\"フォールバックとしてデフォルト辞書での初期化を試みます...\")\n",
        "        wakati_tagger = MeCab.Tagger(f\"-Owakati -r {MECABRC_PATH}\")\n",
        "        wakati_tagger.parse('')\n",
        "        print(\"デフォルト辞書での初期化に成功しました。\")\n",
        "    except Exception as e2:\n",
        "        print(f\"デフォルト辞書での初期化にも失敗しました。エラー: {e2}\")\n",
        "        raise RuntimeError(\"MeCabを正常に初期化できませんでした。\")\n",
        "\n",
        "# IDFデータをファイルから読み込む\n",
        "try:\n",
        "    with open(IDF_DATA_FILE, 'r', encoding='utf-8') as f:\n",
        "        idf_data = json.load(f)\n",
        "    total_documents = idf_data['total_documents'] # Wikipediaの総記事数\n",
        "    doc_freq_map = idf_data['doc_freq_map']       # 各単語が何個の記事に登場したかのデータ\n",
        "    print(f\"'{IDF_DATA_FILE}' の読み込みに成功しました。総文書数: {total_documents}\")\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(f\"エラー: '{IDF_DATA_FILE}' が見つかりません。このスクリプトと同じ場所に配置してください。\")\n",
        "\n",
        "\n",
        "# --- ステップ3: TF (Term Frequency) の計算 ---\n",
        "#\n",
        "# TFは「入力された文章の中で、各単語が何回出現したか」を数えるだけのシンプルな処理です。\n",
        "#\n",
        "print(\"\\n--- TF (Term Frequency) 計算中 ---\")\n",
        "\n",
        "# 1. 入力テキストをMeCabで単語に分割する（分かち書き）\n",
        "tokens = wakati_tagger.parse(INPUT_TEXT).strip().split()\n",
        "\n",
        "# 2. Counterを使って、各単語の出現回数を数える\n",
        "# これが各単語のTFの値（この文章内での出現回数）になります。\n",
        "tf_scores = Counter(tokens)\n",
        "\n",
        "print(\"入力テキスト中の各単語の出現回数 (TF値):\")\n",
        "for word, count in tf_scores.most_common(5):\n",
        "    print(f\"  - '{word}': {count}回\")\n",
        "print(\"  - ...\")\n",
        "\n",
        "\n",
        "# --- ステップ4: IDF (Inverse Document Frequency) の計算 ---\n",
        "#\n",
        "# IDFは「その単語が、世間一般（ここではWikipedia）でどれだけ珍しいか」を数値化する処理です。\n",
        "# 珍しい単語ほど、高いスコアになります。\n",
        "#\n",
        "print(\"\\n--- IDF (Inverse Document Frequency) 計算中 ---\")\n",
        "\n",
        "idf_scores = {}\n",
        "unique_tokens = set(tokens) # 計算を効率化するため、重複しない単語のリストを作成\n",
        "\n",
        "for word in unique_tokens:\n",
        "    # 1. Wikipediaでその単語が登場した記事（文書）の数を取得\n",
        "    #    もし一度も登場していなければ、0として扱う\n",
        "    doc_freq = doc_freq_map.get(word, 0)\n",
        "\n",
        "    # 2. 完全なIDFの計算式を適用\n",
        "    #    log(総文書数 / その単語が登場した文書数)\n",
        "    #    +1 は、分母が0になるのを防ぐ「スムージング」というテクニックです。\n",
        "    idf_value = math.log((total_documents + 1) / (doc_freq + 1))\n",
        "\n",
        "    idf_scores[word] = idf_value\n",
        "\n",
        "print(\"いくつかの単語の「珍しさ」スコア (IDF値):\")\n",
        "# IDFスコアが高い順にソートして上位5件を表示\n",
        "sorted_idf = sorted(idf_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "for word, score in sorted_idf[:5]:\n",
        "    print(f\"  - '{word}': {score:.4f} (珍しい単語ほど高い)\")\n",
        "print(\"  - ...\")\n",
        "\n",
        "\n",
        "# --- ステップ5: TF-IDFスコアの計算と結果表示 ---\n",
        "#\n",
        "# 最後に、TFとIDFを掛け合わせます。\n",
        "# これにより「この文章によく出てきて、かつ世間では珍しい」お宝キーワードがわかります。\n",
        "#\n",
        "print(\"\\n--- TF-IDFスコア計算中 ---\")\n",
        "\n",
        "tfidf_scores = {}\n",
        "for word in unique_tokens:\n",
        "    tf = tf_scores[word]\n",
        "    idf = idf_scores[word]\n",
        "\n",
        "    # TFとIDFを掛け合わせる\n",
        "    tfidf_scores[word] = tf * idf\n",
        "\n",
        "# 結果をTF-IDFスコアの高い順に並べ替えて表示\n",
        "sorted_tfidf = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\n---【最終結果】TF-IDFスコア トップ20 ---\")\n",
        "for word, score in sorted_tfidf[:20]:\n",
        "    print(f\"{word:<10s} | スコア: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP5qYr2t1ZGz",
        "outputId": "5328ba64-81b7-423f-ec03-98873810380b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tfidf.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#前のセル実行\n",
        "!python3.10 tfidf.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x15DBdAJ3bU-",
        "outputId": "b8a37a47-c11a-4497-876b-5d9aab549454"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MeCab Tagger (NEologd) の初期化に成功しました。\n",
            "'wiki_tfidf_data.json' の読み込みに成功しました。総文書数: 2374911\n",
            "\n",
            "--- TF (Term Frequency) 計算中 ---\n",
            "入力テキスト中の各単語の出現回数 (TF値):\n",
            "  - '今日': 1回\n",
            "  - 'は': 1回\n",
            "  - '良い': 1回\n",
            "  - '天気': 1回\n",
            "  - 'です': 1回\n",
            "  - ...\n",
            "\n",
            "--- IDF (Inverse Document Frequency) 計算中 ---\n",
            "いくつかの単語の「珍しさ」スコア (IDF値):\n",
            "  - '天気': 7.1018 (珍しい単語ほど高い)\n",
            "  - 'ね': 4.5772 (珍しい単語ほど高い)\n",
            "  - 'です': 4.5197 (珍しい単語ほど高い)\n",
            "  - '今日': 4.5107 (珍しい単語ほど高い)\n",
            "  - '良い': 4.3670 (珍しい単語ほど高い)\n",
            "  - ...\n",
            "\n",
            "--- TF-IDFスコア計算中 ---\n",
            "\n",
            "---【最終結果】TF-IDFスコア トップ20 ---\n",
            "天気         | スコア: 7.1018\n",
            "ね          | スコア: 4.5772\n",
            "です         | スコア: 4.5197\n",
            "今日         | スコア: 4.5107\n",
            "良い         | スコア: 4.3670\n",
            "は          | スコア: 0.5352\n",
            "。          | スコア: 0.5339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#セル7\n",
        "from google.colab import files\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "print(\"Excelファイル (S2_data.xlsx) と Google Cloud認証キー (JSONファイル) をアップロードしてください。\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# アップロードされたファイル名を取得\n",
        "excel_file_name = \"S2_data.xlsx\"\n",
        "\n",
        "#以下無視\n",
        "gcp_key_file_name = \"\"\n",
        "for fn in uploaded.keys():\n",
        "  if fn.endswith('.json'):\n",
        "    gcp_key_file_name = fn\n",
        "    print(f\"認証キー '{fn}' を認識しました。\")\n",
        "\n",
        "if not gcp_key_file_name:\n",
        "    print(\"エラー: Google Cloudの認証キー(JSONファイル)が見つかりません。\")\n",
        "\n",
        "# 環境変数を設定してPythonから認証キーを使えるようにする\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = gcp_key_file_name"
      ],
      "metadata": {
        "id": "zcU1lFXn6Hyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#セル8.1\n",
        "#セル5.1対応\n",
        "%%writefile create_json.py\n",
        "import pandas as pd\n",
        "import MeCab\n",
        "from collections import Counter, defaultdict\n",
        "import json\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "from google.cloud import translate_v2 as translate\n",
        "import html\n",
        "\n",
        "# --- 設定項目 ---\n",
        "# 1. MeCabの辞書パス:下記\n",
        "\n",
        "# 2. ジャンルIDマップ\n",
        "GENRE_ID_MAP = {\n",
        "    \"文芸・評論\": \"bungei\", \"ノンフィクション\": \"nonfiction\", \"ビジネス・経済\": \"business\",\n",
        "    \"歴史・地理\": \"rekishi\", \"政治・社会\": \"politics\", \"芸能・エンターテインメント\": \"entertainment\",\n",
        "    \"アート・建築・デザイン\": \"art\", \"人文・思想・宗教\": \"humanities\", \"暮らし・健康・料理\": \"life\",\n",
        "    \"サイエンス・テクノロジー\": \"science\", \"趣味・実用\": \"hobby\", \"教育・自己啓発\": \"education\",\n",
        "    \"スポーツ・アウトドア\": \"sports\", \"事典・年鑑・本・ことば\": \"dictionary\", \"音楽\": \"music\",\n",
        "    \"旅行・紀行\": \"ryokou\", \"絵本・児童書\": \"kids\", \"コミックス\": \"comics\"\n",
        "}\n",
        "\n",
        "# 3. Google CloudプロジェクトID\n",
        "GCP_PROJECT_ID = \"my-translation-app-463407\"  # ご自身のIDに設定してください\n",
        "\n",
        "# 4. 文の翻訳を有効にするか\n",
        "TRANSLATE_SENTENCES = True\n",
        "\n",
        "# 5. ★★★ 読み込むファイルを「完全なIDF」用のデータファイルに変更 ★★★\n",
        "WIKI_IDF_FILE = \"wiki_tfidf_data.json\"\n",
        "\n",
        "# 6. ファイル設定\n",
        "EXCEL_FILE = \"S2_data.xlsx\"\n",
        "SHEET_NAMES = list(GENRE_ID_MAP.keys())\n",
        "OUTPUT_JSON_FILE = \"analysis_data.json\"\n",
        "ROW_START, ROW_END = 1, 11\n",
        "\n",
        "# 7. Softmax温度設定\n",
        "SOFTMAX_TEMPERATURE = 15.0 # なだらかな分布にするため少し高めに設定\n",
        "\n",
        "# NEologd辞書と、mecabrc設定ファイルのパスを明示的に指定します。\n",
        "NEOLOGD_DIC_PATH = \"/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
        "MECABRC_PATH = \"/etc/mecabrc\"\n",
        "\n",
        "try:\n",
        "    # 辞書パスと設定ファイルパスの両方を指定\n",
        "    tagger_options = f\"-d {NEOLOGD_DIC_PATH} -r {MECABRC_PATH}\"\n",
        "    wakati_tagger = MeCab.Tagger(f\"-Owakati {tagger_options}\")\n",
        "    wakati_tagger.parse('') # 初期化チェック\n",
        "    print(\"MeCab Tagger (NEologd) の初期化に成功しました。\")\n",
        "except Exception as e:\n",
        "    print(f\"MeCab Tagger (NEologd) の初期化に失敗しました。エラー: {e}\")\n",
        "    # フォールバックもrcfileを指定\n",
        "    try:\n",
        "        print(\"フォールバックとしてデフォルト辞書での初期化を試みます...\")\n",
        "        wakati_tagger = MeCab.Tagger(f\"-Owakati -r {MECABRC_PATH}\")\n",
        "        wakati_tagger.parse('')\n",
        "        print(\"デフォルト辞書での初期化に成功しました。\")\n",
        "    except Exception as e2:\n",
        "        print(f\"デフォルト辞書での初期化にも失敗しました。エラー: {e2}\")\n",
        "        raise RuntimeError(\"MeCabを正常に初期化できませんでした。\")\n",
        "\n",
        "# --- 関数定義 ---\n",
        "# ★★★ 修正: 新しいJSON構造に合わせてデータ読み込み関数を修正 ★★★\n",
        "def load_idf_data(filepath):\n",
        "    \"\"\"文書頻度データ（JSON）を読み込む\"\"\"\n",
        "    print(f\"ステップ1/5: 文書頻度データ '{filepath}' を読み込み中...\")\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"  - エラー: ファイル '{filepath}' が見つかりません。\")\n",
        "        exit()\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    if 'total_documents' not in data or 'doc_freq_map' not in data:\n",
        "        print(f\"  - エラー: '{filepath}' は 'total_documents' と 'doc_freq_map' のキーを持つ必要があります。\")\n",
        "        exit()\n",
        "    print(f\"  - 完了。{len(data['doc_freq_map'])}語のデータを読み込みました。\")\n",
        "    return data['doc_freq_map'], data['total_documents']\n",
        "\n",
        "def translate_texts(texts, target_language=\"en\"):\n",
        "    \"\"\"テキストのリストを翻訳し、HTMLエンティティをデコードする\"\"\"\n",
        "    if not texts: return {}\n",
        "    unique_texts = list(set(text for text in texts if text and isinstance(text, str)))\n",
        "    if not unique_texts: return {text: \"\" for text in texts}\n",
        "\n",
        "    BATCH_SIZE, all_translations = 128, {}\n",
        "    print(f\"  - 合計 {len(unique_texts)}件のユニークなテキストを翻訳します...\")\n",
        "    for i in range(0, len(unique_texts), BATCH_SIZE):\n",
        "        chunk = unique_texts[i:i + BATCH_SIZE]\n",
        "        print(f\"    - 翻訳中... (チャンク {i//BATCH_SIZE + 1} / {math.ceil(len(unique_texts)/BATCH_SIZE)})\")\n",
        "        try:\n",
        "            results = translate_client.translate(chunk, target_language=target_language)\n",
        "            for item in results:\n",
        "                decoded_text = html.unescape(item['translatedText'])\n",
        "                all_translations[item['input']] = decoded_text\n",
        "        except Exception as e:\n",
        "            print(f\"      - 翻訳API呼び出し中にエラー: {e}\")\n",
        "            for text in chunk: all_translations[text] = \"\"\n",
        "        time.sleep(0.1)\n",
        "    return all_translations\n",
        "\n",
        "def apply_softmax(word_objects, temperature=1.0):\n",
        "    if not word_objects: return []\n",
        "    if temperature <= 0: temperature = 1.0\n",
        "    scores = [item['score'] / temperature for item in word_objects]\n",
        "    max_score = max(scores)\n",
        "    exps = [math.exp(s - max_score) for s in scores]\n",
        "    sum_exps = sum(exps)\n",
        "    if sum_exps == 0: return word_objects\n",
        "    for i, item in enumerate(word_objects):\n",
        "        item['score'] = exps[i] / sum_exps\n",
        "    return word_objects\n",
        "\n",
        "def get_tokens_from_text(text):\n",
        "    if not isinstance(text, str): text = str(text)\n",
        "    return wakati_tagger.parse(text).strip().split()\n",
        "\n",
        "# ★★★ 修正: 完全なIDFを計算するように関数を修正 ★★★\n",
        "def analyze_pos_and_score(text, doc_freq_map, total_documents):\n",
        "    \"\"\"テキストを形態素解析し、品詞分類とTF-IDFスコアリングを行う\"\"\"\n",
        "    if not isinstance(text, str): text = str(text)\n",
        "\n",
        "    tokens_for_tf = get_tokens_from_text(text)\n",
        "    tf = Counter(tokens_for_tf)\n",
        "    node = tagger.parseToNode(text)\n",
        "    results = defaultdict(list)\n",
        "\n",
        "    while node:\n",
        "        surface = node.surface\n",
        "        features = node.feature.split(',')\n",
        "        if not surface or len(features) < 2:\n",
        "            node = node.next\n",
        "            continue\n",
        "\n",
        "        freq_in_doc = tf.get(surface, 0)\n",
        "        if freq_in_doc == 0:\n",
        "            node = node.next\n",
        "            continue\n",
        "\n",
        "        # doc_freq_mapからその単語の文書頻度を取得\n",
        "        doc_freq = doc_freq_map.get(surface, 0)\n",
        "\n",
        "        # 完全なIDFの計算式（+1はゼロ除算防止のスムージング）\n",
        "        idf = math.log((total_documents + 1) / (doc_freq + 1))\n",
        "\n",
        "        score = (freq_in_doc * idf) + math.log(freq_in_doc + 1)\n",
        "        word_obj = {\"word\": surface, \"score\": score}\n",
        "\n",
        "        pos, sub_pos1 = features[0], features[1]\n",
        "\n",
        "        if pos == \"名詞\":\n",
        "            if sub_pos1 in [\"代名詞\", \"非自立\", \"数\", \"接尾\", \"副詞可能\", \"形容動詞語幹\"]:\n",
        "                results[\"nouns_functional\"].append(word_obj)\n",
        "            else:\n",
        "                results[\"nouns_other\"].append(word_obj)\n",
        "        elif pos == \"動詞\": results[\"verbs\"].append(word_obj)\n",
        "        elif pos == \"形容詞\": results[\"adjectives\"].append(word_obj)\n",
        "        elif pos == \"形容動詞\": results[\"adjectival_verbs\"].append(word_obj)\n",
        "\n",
        "        node = node.next\n",
        "    return results\n",
        "\n",
        "# ★★★ 削除: 不要になったため、extract_important_words関数を削除 ★★★\n",
        "\n",
        "def extract_important_sentences_by_score(text, word_score_map):\n",
        "    if not isinstance(text, str): return \"説明文がテキスト形式ではありません\"\n",
        "    sentences = [s.strip() for s in text.split(\"。\") if s.strip()]\n",
        "    if not sentences: return \"該当する文がありません\"\n",
        "\n",
        "    scored_sentences = []\n",
        "    for sentence in sentences:\n",
        "        tokens = get_tokens_from_text(sentence)\n",
        "        sentence_score = sum(word_score_map.get(token, 0) for token in tokens)\n",
        "        scored_sentences.append((sentence, sentence_score))\n",
        "\n",
        "    scored_sentences = [s for s in scored_sentences if s[1] > 0]\n",
        "    if not scored_sentences: return \"重要度の高い単語を含む文がありません\"\n",
        "\n",
        "    scored_sentences.sort(key=lambda x: x[1], reverse=True)\n",
        "    return scored_sentences[0][0] + \"。\"\n",
        "\n",
        "# --- メイン処理 ---\n",
        "def main():\n",
        "    # ★★★ 修正: 新しいデータ読み込み関数を呼び出し ★★★\n",
        "    doc_freq_map, total_documents = load_idf_data(WIKI_IDF_FILE)\n",
        "\n",
        "    print(\"ステップ2/5: 全てのドキュメントをExcelから読み込み中...\")\n",
        "    all_docs_raw = []\n",
        "    for sheet_name in SHEET_NAMES:\n",
        "        try:\n",
        "            df = pd.read_excel(EXCEL_FILE, sheet_name=sheet_name, usecols=[1, 3], header=None, engine=\"openpyxl\")\n",
        "            for i in range(ROW_START, ROW_END):\n",
        "                if i < len(df):\n",
        "                    all_docs_raw.append({\n",
        "                        \"sheet_name\": sheet_name,\n",
        "                        \"title\": str(df.iloc[i, 0]),\n",
        "                        \"text\": str(df.iloc[i, 1])\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            print(f\"  - シート '{sheet_name}' の読み込みエラー: {e}\")\n",
        "\n",
        "    print(\"ステップ3/5: 各ドキュメントを解析し、スコアを計算中...\")\n",
        "    all_docs_processed = []\n",
        "    for i, doc in enumerate(all_docs_raw):\n",
        "        # ★★★ 修正: 新しい引数で関数を呼び出し ★★★\n",
        "        analysis_results_raw = analyze_pos_and_score(doc['text'], doc_freq_map, total_documents)\n",
        "\n",
        "        aggregated_analysis = defaultdict(lambda: defaultdict(float))\n",
        "        word_score_map = {}\n",
        "        for category, word_list in analysis_results_raw.items():\n",
        "            for item in word_list: aggregated_analysis[category][item['word']] += item['score']\n",
        "        for category_scores in aggregated_analysis.values():\n",
        "            for word, score in category_scores.items(): word_score_map[word] = score\n",
        "\n",
        "        final_analysis_results = {\n",
        "            cat: apply_softmax([{\"word\": w, \"score\": s} for w, s in scores.items()], temperature=SOFTMAX_TEMPERATURE)\n",
        "            for cat, scores in aggregated_analysis.items()\n",
        "        }\n",
        "\n",
        "        # ★★★ 削除: 重要単語に関する処理をすべて削除 ★★★\n",
        "\n",
        "        doc.update({\n",
        "            \"genre_id\": GENRE_ID_MAP.get(doc['sheet_name'], doc['sheet_name'].lower()),\n",
        "            \"title_id\": f\"title{(i % (ROW_END - ROW_START)) + 1:02d}\",\n",
        "            \"important_sentence\": extract_important_sentences_by_score(doc['text'], word_score_map),\n",
        "            \"analysis\": final_analysis_results,\n",
        "            # 'important_words' キーは追加しない\n",
        "        })\n",
        "        all_docs_processed.append(doc)\n",
        "\n",
        "    print(\"ステップ4/5: テキストを翻訳中...\")\n",
        "    words_to_translate, sentences_to_translate = set(), []\n",
        "    for doc in all_docs_processed:\n",
        "        if TRANSLATE_SENTENCES:\n",
        "            sentences_to_translate.append(doc['text'])\n",
        "            sentences_to_translate.append(doc['important_sentence'])\n",
        "        for category in doc['analysis']:\n",
        "            for item in doc['analysis'][category]:\n",
        "                words_to_translate.add(item['word'])\n",
        "        # ★★★ 削除: 重要単語の翻訳処理を削除 ★★★\n",
        "\n",
        "    word_translation_map = translate_texts(list(words_to_translate))\n",
        "    sentence_translation_map = translate_texts(sentences_to_translate) if TRANSLATE_SENTENCES else {}\n",
        "\n",
        "    print(\"ステップ5/5: 最終的なJSONデータを構築中...\")\n",
        "    final_data = defaultdict(list)\n",
        "    for doc in all_docs_processed:\n",
        "        for category in doc['analysis']:\n",
        "            for item in doc['analysis'][category]:\n",
        "                item['translation'] = word_translation_map.get(item['word'], \"\")\n",
        "        # ★★★ 削除: 重要単語の翻訳マッピング処理を削除 ★★★\n",
        "\n",
        "        doc['text_en'] = sentence_translation_map.get(doc['text'], \"\")\n",
        "        doc['important_sentence_en'] = sentence_translation_map.get(doc['important_sentence'], \"\")\n",
        "\n",
        "        doc.pop('sheet_name', None)\n",
        "        final_data[doc['genre_id']].append(doc)\n",
        "\n",
        "    id_to_genre_map = {v: k for k, v in GENRE_ID_MAP.items()}\n",
        "    final_json_data = {id_to_genre_map.get(gid, gid): docs for gid, docs in final_data.items()}\n",
        "\n",
        "    with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f:\n",
        "        json.dump(final_json_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"\\n処理完了。'{OUTPUT_JSON_FILE}' を保存しました。\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "p1ei6HMP2Y_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#セル8.2\n",
        "#セル5.2対応\n",
        "%%writefile create_json.py\n",
        "import pandas as pd\n",
        "import MeCab\n",
        "from collections import Counter, defaultdict\n",
        "import json\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "from google.cloud import translate_v2 as translate\n",
        "import html # ★★★ HTMLデコードライブラリをインポート ★★★\n",
        "\n",
        "# --- 設定項目 ---\n",
        "# 1. MeCabの辞書パス\n",
        "MECAB_DIC_PATH = \"/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
        "\n",
        "# 2. ジャンルIDマップ。\n",
        "GENRE_ID_MAP = {\n",
        "    \"文芸・評論\": \"bungei\", \"ノンフィクション\": \"nonfiction\", \"ビジネス・経済\": \"business\",\n",
        "    \"歴史・地理\": \"rekishi\", \"政治・社会\": \"politics\", \"芸能・エンターテインメント\": \"entertainment\",\n",
        "    \"アート・建築・デザイン\": \"art\", \"人文・思想・宗教\": \"humanities\", \"暮らし・健康・料理\": \"life\",\n",
        "    \"サイエンス・テクノロジー\": \"science\", \"趣味・実用\": \"hobby\", \"教育・自己啓発\": \"education\",\n",
        "    \"スポーツ・アウトドア\": \"sports\", \"事典・年鑑・本・ことば\": \"dictionary\", \"音楽\": \"music\",\n",
        "    \"旅行・紀行\": \"ryokou\", \"絵本・児童書\": \"kids\", \"コミックス\": \"comics\"\n",
        "}\n",
        "\n",
        "# 3. Google CloudプロジェクトID。\n",
        "GCP_PROJECT_ID = \"my-translation-app-463407\"  # ご自身のIDに設定してください\n",
        "\n",
        "# 4. 文の翻訳を有効にするか。\n",
        "TRANSLATE_SENTENCES = True\n",
        "\n",
        "# 5. 読み込む頻度データファイル。\n",
        "WIKI_FREQ_FILE = \"wiki_freq_neologd.json\"\n",
        "\n",
        "# 6. ファイル設定\n",
        "EXCEL_FILE = \"S2_data.xlsx\"\n",
        "SHEET_NAMES = list(GENRE_ID_MAP.keys())\n",
        "OUTPUT_JSON_FILE = \"analysis_data.json\"\n",
        "ROW_START, ROW_END = 1, 11\n",
        "\n",
        "# 7. Softmax温度設定。\n",
        "SOFTMAX_TEMPERATURE = 15.0\n",
        "\n",
        "# --- クライアントとTaggerの初期化 ---\n",
        "try:\n",
        "    translate_client = translate.Client()\n",
        "except Exception as e:\n",
        "    print(f\"Google翻訳クライアントの初期化に失敗。認証情報を確認してください。エラー: {e}\")\n",
        "    exit()\n",
        "\n",
        "try:\n",
        "    tagger_options = f'-d \"{MECAB_DIC_PATH}\"'\n",
        "    tagger = MeCab.Tagger(tagger_options)\n",
        "    wakati_tagger = MeCab.Tagger(f\"-Owakati {tagger_options}\")\n",
        "    tagger.parse('')\n",
        "    print(\"MeCab Tagger (NEologd) の初期化に成功しました。\")\n",
        "except RuntimeError as e:\n",
        "    print(f\"MeCabの初期化に失敗しました。: {e}\")\n",
        "    print(f\"MECAB_DIC_PATH '{MECAB_DIC_PATH}' が正しいか確認してください。\")\n",
        "    exit()\n",
        "\n",
        "# --- 関数定義 ---\n",
        "def load_freq_data(filepath):\n",
        "    print(f\"ステップ1/5: 外部単語頻度データ '{filepath}' を読み込み中...\")\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"  - エラー: ファイル '{filepath}' が見つかりません。\")\n",
        "        exit()\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    if 'freq_map' not in data or 'total_words' not in data:\n",
        "        print(f\"  - エラー: '{filepath}' は 'freq_map' と 'total_words' のキーを持つ必要があります。\")\n",
        "        exit()\n",
        "    print(f\"  - 完了。{len(data['freq_map'])}語のデータを読み込みました。\")\n",
        "    return data['freq_map'], data['total_words']\n",
        "\n",
        "def translate_texts(texts, target_language=\"en\"):\n",
        "    \"\"\"テキストのリストを翻訳し、HTMLエンティティをデコードする\"\"\"\n",
        "    if not texts: return {}\n",
        "    unique_texts = list(set(text for text in texts if text and isinstance(text, str)))\n",
        "    if not unique_texts: return {text: \"\" for text in texts}\n",
        "\n",
        "    BATCH_SIZE, all_translations = 128, {}\n",
        "    print(f\"  - 合計 {len(unique_texts)}件のユニークなテキストを翻訳します...\")\n",
        "    for i in range(0, len(unique_texts), BATCH_SIZE):\n",
        "        chunk = unique_texts[i:i + BATCH_SIZE]\n",
        "        print(f\"    - 翻訳中... (チャンク {i//BATCH_SIZE + 1} / {math.ceil(len(unique_texts)/BATCH_SIZE)})\")\n",
        "        try:\n",
        "            results = translate_client.translate(chunk, target_language=target_language)\n",
        "            # ★★★ HTMLエンティティをデコードする処理を追加 ★★★\n",
        "            for item in results:\n",
        "                decoded_text = html.unescape(item['translatedText'])\n",
        "                all_translations[item['input']] = decoded_text\n",
        "        except Exception as e:\n",
        "            print(f\"      - 翻訳API呼び出し中にエラー: {e}\")\n",
        "            for text in chunk: all_translations[text] = \"\"\n",
        "        time.sleep(0.1)\n",
        "    return all_translations\n",
        "\n",
        "def apply_softmax(word_objects, temperature=1.0):\n",
        "    if not word_objects: return []\n",
        "    if temperature <= 0: temperature = 1.0\n",
        "    scores = [item['score'] / temperature for item in word_objects]\n",
        "    max_score = max(scores)\n",
        "    exps = [math.exp(s - max_score) for s in scores]\n",
        "    sum_exps = sum(exps)\n",
        "    if sum_exps == 0: return word_objects\n",
        "    for i, item in enumerate(word_objects):\n",
        "        item['score'] = exps[i] / sum_exps\n",
        "    return word_objects\n",
        "\n",
        "def get_tokens_from_text(text):\n",
        "    if not isinstance(text, str): text = str(text)\n",
        "    return wakati_tagger.parse(text).strip().split()\n",
        "\n",
        "def analyze_pos_and_score(text, freq_map, total_corpus_words):\n",
        "    if not isinstance(text, str): text = str(text)\n",
        "\n",
        "    tokens_for_tf = get_tokens_from_text(text)\n",
        "    tf = Counter(tokens_for_tf)\n",
        "    node = tagger.parseToNode(text)\n",
        "    results = defaultdict(list)\n",
        "\n",
        "    max_idf = math.log((total_corpus_words / 1) + 1)\n",
        "\n",
        "    while node:\n",
        "        surface = node.surface\n",
        "        features = node.feature.split(',')\n",
        "        if not surface or len(features) < 2:\n",
        "            node = node.next\n",
        "            continue\n",
        "\n",
        "        freq_in_doc = tf.get(surface, 0)\n",
        "        if freq_in_doc == 0:\n",
        "            node = node.next\n",
        "            continue\n",
        "\n",
        "        freq_in_corpus = freq_map.get(surface)\n",
        "        idf = max_idf if freq_in_corpus is None else math.log((total_corpus_words / (freq_in_corpus + 1)) + 1)\n",
        "\n",
        "        score = (freq_in_doc * idf) + math.log(freq_in_doc + 1)\n",
        "        word_obj = {\"word\": surface, \"score\": score}\n",
        "\n",
        "        pos, sub_pos1 = features[0], features[1]\n",
        "\n",
        "        if pos == \"名詞\":\n",
        "            if sub_pos1 in [\"代名詞\", \"非自立\", \"数\", \"接尾\", \"副詞可能\", \"形容動詞語幹\"]:\n",
        "                results[\"nouns_functional\"].append(word_obj)\n",
        "            else:\n",
        "                results[\"nouns_other\"].append(word_obj)\n",
        "        elif pos == \"動詞\":\n",
        "            results[\"verbs\"].append(word_obj)\n",
        "        elif pos == \"形容詞\":\n",
        "            results[\"adjectives\"].append(word_obj)\n",
        "        elif pos == \"形容動詞\":\n",
        "            results[\"adjectival_verbs\"].append(word_obj)\n",
        "\n",
        "        node = node.next\n",
        "    return results\n",
        "\n",
        "def extract_important_words(analysis_results_raw):\n",
        "    important = defaultdict(list)\n",
        "    for pos_key, words in analysis_results_raw.items():\n",
        "        word_counts = Counter(item['word'] for item in words)\n",
        "        important_word_names = {word for word, count in word_counts.items() if count >= 2}\n",
        "\n",
        "        important_words = [item for item in words if item['word'] in important_word_names]\n",
        "        important[pos_key] = important_words\n",
        "    return important\n",
        "\n",
        "def extract_important_sentences_by_score(text, word_score_map):\n",
        "    if not isinstance(text, str): return \"説明文がテキスト形式ではありません\"\n",
        "\n",
        "    sentences = [s.strip() for s in text.split(\"。\") if s.strip()]\n",
        "    if not sentences: return \"該当する文がありません\"\n",
        "\n",
        "    scored_sentences = []\n",
        "    for sentence in sentences:\n",
        "        tokens = get_tokens_from_text(sentence)\n",
        "        sentence_score = sum(word_score_map.get(token, 0) for token in tokens)\n",
        "        scored_sentences.append((sentence, sentence_score))\n",
        "\n",
        "    scored_sentences = [s for s in scored_sentences if s[1] > 0]\n",
        "    if not scored_sentences: return \"重要度の高い単語を含む文がありません\"\n",
        "\n",
        "    scored_sentences.sort(key=lambda x: x[1], reverse=True)\n",
        "    return scored_sentences[0][0] + \"。\"\n",
        "\n",
        "# --- メイン処理 ---\n",
        "def main():\n",
        "    freq_map, total_words = load_freq_data(WIKI_FREQ_FILE)\n",
        "\n",
        "    print(\"ステップ2/5: 全てのドキュメントをExcelから読み込み中...\")\n",
        "    all_docs_raw = []\n",
        "    for sheet_name in SHEET_NAMES:\n",
        "        try:\n",
        "            df = pd.read_excel(EXCEL_FILE, sheet_name=sheet_name, usecols=[1, 3], header=None, engine=\"openpyxl\")\n",
        "            for i in range(ROW_START, ROW_END):\n",
        "                if i < len(df):\n",
        "                    all_docs_raw.append({\n",
        "                        \"sheet_name\": sheet_name,\n",
        "                        \"title\": str(df.iloc[i, 0]),\n",
        "                        \"text\": str(df.iloc[i, 1])\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            print(f\"  - シート '{sheet_name}' の読み込みエラー: {e}\")\n",
        "\n",
        "    print(\"ステップ3/5: 各ドキュメントを解析し、スコアを計算中...\")\n",
        "    all_docs_processed = []\n",
        "    for i, doc in enumerate(all_docs_raw):\n",
        "        analysis_results_raw = analyze_pos_and_score(doc['text'], freq_map, total_words)\n",
        "\n",
        "        aggregated_analysis = defaultdict(lambda: defaultdict(float))\n",
        "        word_score_map = {}\n",
        "        for category, word_list in analysis_results_raw.items():\n",
        "            for item in word_list:\n",
        "                aggregated_analysis[category][item['word']] += item['score']\n",
        "        for category_scores in aggregated_analysis.values():\n",
        "            for word, score in category_scores.items():\n",
        "                word_score_map[word] = score\n",
        "\n",
        "        final_analysis_results = {\n",
        "            cat: apply_softmax([{\"word\": w, \"score\": s} for w, s in scores.items()], temperature=SOFTMAX_TEMPERATURE)\n",
        "            for cat, scores in aggregated_analysis.items()\n",
        "        }\n",
        "\n",
        "        important_words_raw = extract_important_words(analysis_results_raw)\n",
        "        aggregated_important = defaultdict(lambda: defaultdict(float))\n",
        "        for category, word_list in important_words_raw.items():\n",
        "            for item in word_list:\n",
        "                aggregated_important[category][item['word']] += item['score']\n",
        "\n",
        "        final_important_words = {\n",
        "            cat: apply_softmax([{\"word\": w, \"score\": s} for w, s in scores.items()], temperature=SOFTMAX_TEMPERATURE)\n",
        "            for cat, scores in aggregated_important.items()\n",
        "        }\n",
        "\n",
        "        doc.update({\n",
        "            \"genre_id\": GENRE_ID_MAP.get(doc['sheet_name'], doc['sheet_name'].lower()),\n",
        "            \"title_id\": f\"title{(i % (ROW_END - ROW_START)) + 1:02d}\",\n",
        "            \"important_sentence\": extract_important_sentences_by_score(doc['text'], word_score_map),\n",
        "            \"analysis\": final_analysis_results,\n",
        "            \"important_words\": final_important_words\n",
        "        })\n",
        "        all_docs_processed.append(doc)\n",
        "\n",
        "    print(\"ステップ4/5: テキストを翻訳中...\")\n",
        "    words_to_translate, sentences_to_translate = set(), []\n",
        "    for doc in all_docs_processed:\n",
        "        if TRANSLATE_SENTENCES:\n",
        "            sentences_to_translate.append(doc['text'])\n",
        "            sentences_to_translate.append(doc['important_sentence'])\n",
        "        for category in doc['analysis']:\n",
        "            for item in doc['analysis'][category]:\n",
        "                words_to_translate.add(item['word'])\n",
        "        for category in doc['important_words']:\n",
        "            for item in doc['important_words'][category]:\n",
        "                words_to_translate.add(item['word'])\n",
        "\n",
        "    word_translation_map = translate_texts(list(words_to_translate))\n",
        "    sentence_translation_map = translate_texts(sentences_to_translate) if TRANSLATE_SENTENCES else {}\n",
        "\n",
        "    print(\"ステップ5/5: 最終的なJSONデータを構築中...\")\n",
        "    final_data = defaultdict(list)\n",
        "    for doc in all_docs_processed:\n",
        "        for category in doc['analysis']:\n",
        "            for item in doc['analysis'][category]:\n",
        "                item['translation'] = word_translation_map.get(item['word'], \"\")\n",
        "        for category in doc['important_words']:\n",
        "            for item in doc['important_words'][category]:\n",
        "                item['translation'] = word_translation_map.get(item['word'], \"\")\n",
        "\n",
        "        doc['text_en'] = sentence_translation_map.get(doc['text'], \"\")\n",
        "        doc['important_sentence_en'] = sentence_translation_map.get(doc['important_sentence'], \"\")\n",
        "\n",
        "        doc.pop('sheet_name', None)\n",
        "        final_data[doc['genre_id']].append(doc)\n",
        "\n",
        "    id_to_genre_map = {v: k for k, v in GENRE_ID_MAP.items()}\n",
        "    final_json_data = {id_to_genre_map.get(gid, gid): docs for gid, docs in final_data.items()}\n",
        "\n",
        "    with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f:\n",
        "        json.dump(final_json_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"\\n処理完了。'{OUTPUT_JSON_FILE}' を保存しました。\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "muGCi25hXGBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#セル9\n",
        "#前のセル実行\n",
        "!python3.10 create_json.py"
      ],
      "metadata": {
        "id": "whzqZUp8XTa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r extracted_wiki /content/drive/MyDrive/\n",
        "!cp -r jawiki-latest-pages-articles.xml.bz2 /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "BqgPH0alHFoo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}